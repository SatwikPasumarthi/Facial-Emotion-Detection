{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17de2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b698c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_image=[\"train\",\"test\"]\n",
    "categories=[\"angry\",\"disgust\",\"fear\",\"happy\",\"neutral\",\"sad\",\"surprise\"]\n",
    "category_index={\"angry\":0,\"disgust\":1,\"fear\":2,\"happy\":3,\"neutral\":4,\"sad\":5,\"surprise\":6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e693fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]    #the values of trained images are stored in this array\n",
    "y_train=[]    #the indexes of trained images that are beloged to their category are stored in this array\n",
    "x_test=[]     #the values of tested images are stored in this array\n",
    "y_test=[]     #the indexes of that tested images that are beloged to their category are stored in this array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145dffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"C:\\\\Users\\\\pasum\\\\Desktop\\\\Emotion Detector\\\\archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4801a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_resizing(img_path):\n",
    "    img=cv2.imread(img_path)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    img=cv2.resize(img,(48,48))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14c3ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the train images/.......\n",
      "    loading images of the category angry in train images.......\n",
      "    loading images of the category disgust in train images.......\n",
      "    loading images of the category fear in train images.......\n",
      "    loading images of the category happy in train images.......\n",
      "    loading images of the category neutral in train images.......\n",
      "    loading images of the category sad in train images.......\n",
      "    loading images of the category surprise in train images.......\n",
      "Extracting the test images/.......\n",
      "    loading images of the category angry in test images.......\n",
      "    loading images of the category disgust in test images.......\n",
      "    loading images of the category fear in test images.......\n",
      "    loading images of the category happy in test images.......\n",
      "    loading images of the category neutral in test images.......\n",
      "    loading images of the category sad in test images.......\n",
      "    loading images of the category surprise in test images.......\n",
      "Fetching data completed........\n"
     ]
    }
   ],
   "source": [
    "for classify_img in classified_image:\n",
    "    \n",
    "    path1=os.path.join(data_path,classify_img)   #path1=C:\\Users\\pasum\\Desktop\\Emotion Detector\\archive\\....(i.e test or train)\n",
    "    print(\"Extracting the \"+classify_img+\" images/.......\")\n",
    "    \n",
    "    for category in categories:\n",
    "        folder_path=os.path.join(path1,category) #folder_path=C:\\Users\\pasum\\Desktop\\Emotion Detector\\archive\\(i.e test or train)\n",
    "        images=os.listdir(folder_path)\n",
    "        \n",
    "        print(f'    loading images of the category {category} in {classify_img} images.......')\n",
    "        \n",
    "        for image in images:\n",
    "            img_path=os.path.join(folder_path,image)\n",
    "            try:\n",
    "                resized_img=img_resizing(img_path)\n",
    "                if classify_img==\"train\":\n",
    "                    x_train.append(resized_img)\n",
    "                    y_train.append(category_index[category])\n",
    "                else:\n",
    "                    x_test.append(resized_img)\n",
    "                    y_test.append(category_index[category])\n",
    "                    \n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(\"Fetching data completed........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8603e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array(x_train)\n",
    "y_train=np.array(y_train)\n",
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39895a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7070aedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9095bdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178, 48, 48)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b35827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d8c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 7 emotions so..\n",
    "numOfCategories=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cc227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea51b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Althogh when we converted to grayscale the imgs still have 3 color channels(BGR),so we have to RESHAPE it\n",
    "x_train=np.reshape(x_train,(x_train.shape[0],48,48,1))\n",
    "x_test=np.reshape(x_test,(x_test.shape[0],48,48,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eac228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train,numOfCategories)\n",
    "y_test=to_categorical(y_test,numOfCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "106f702f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "033666b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a016fab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178, 48, 48, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cc02b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dcc9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c96ecd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fb6ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),input_shape=(48,48,1),activation=\"elu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,kernel_size=(3,3),input_shape=(48,48,1),activation=\"elu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),activation=\"elu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,kernel_size=(3,3),activation=\"elu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256,kernel_size=(3,3),activation=\"elu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,kernel_size=(3,3),activation=\"elu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(32,activation=\"relu\"))\n",
    "model.add(Dense(7,activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0e3df8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 1,289,607\n",
      "Trainable params: 1,287,815\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d3ffe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22ed2b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "575/575 [==============================] - 341s 594ms/step - loss: 1.6184 - accuracy: 0.3558 - val_loss: 2.9630 - val_accuracy: 0.3050\n",
      "Epoch 2/20\n",
      "575/575 [==============================] - 347s 604ms/step - loss: 1.3676 - accuracy: 0.4717 - val_loss: 1.2706 - val_accuracy: 0.5183\n",
      "Epoch 3/20\n",
      "575/575 [==============================] - 392s 681ms/step - loss: 1.2511 - accuracy: 0.5237 - val_loss: 1.3485 - val_accuracy: 0.4876\n",
      "Epoch 4/20\n",
      "575/575 [==============================] - 375s 652ms/step - loss: 1.1752 - accuracy: 0.5549 - val_loss: 1.1845 - val_accuracy: 0.5573\n",
      "Epoch 5/20\n",
      "575/575 [==============================] - 359s 624ms/step - loss: 1.1166 - accuracy: 0.5778 - val_loss: 1.1834 - val_accuracy: 0.5571\n",
      "Epoch 6/20\n",
      "575/575 [==============================] - 363s 631ms/step - loss: 1.0635 - accuracy: 0.6010 - val_loss: 1.0732 - val_accuracy: 0.5995\n",
      "Epoch 7/20\n",
      "575/575 [==============================] - 355s 618ms/step - loss: 1.0117 - accuracy: 0.6191 - val_loss: 1.1064 - val_accuracy: 0.5901\n",
      "Epoch 8/20\n",
      "575/575 [==============================] - 362s 629ms/step - loss: 0.9645 - accuracy: 0.6365 - val_loss: 1.0823 - val_accuracy: 0.5947\n",
      "Epoch 9/20\n",
      "575/575 [==============================] - 395s 686ms/step - loss: 0.9145 - accuracy: 0.6562 - val_loss: 1.1158 - val_accuracy: 0.5933\n",
      "Epoch 10/20\n",
      "575/575 [==============================] - 395s 686ms/step - loss: 0.8760 - accuracy: 0.6709 - val_loss: 1.2609 - val_accuracy: 0.5543\n",
      "Epoch 11/20\n",
      "575/575 [==============================] - 403s 701ms/step - loss: 0.8330 - accuracy: 0.6899 - val_loss: 1.1090 - val_accuracy: 0.5975\n",
      "Epoch 12/20\n",
      "575/575 [==============================] - 391s 680ms/step - loss: 0.7893 - accuracy: 0.7072 - val_loss: 1.0520 - val_accuracy: 0.6233\n",
      "Epoch 13/20\n",
      "575/575 [==============================] - 442s 768ms/step - loss: 0.7524 - accuracy: 0.7228 - val_loss: 1.1208 - val_accuracy: 0.6023\n",
      "Epoch 14/20\n",
      "575/575 [==============================] - 387s 672ms/step - loss: 0.7133 - accuracy: 0.7364 - val_loss: 1.1364 - val_accuracy: 0.6099\n",
      "Epoch 15/20\n",
      "575/575 [==============================] - 433s 754ms/step - loss: 0.6824 - accuracy: 0.7493 - val_loss: 1.0752 - val_accuracy: 0.6307\n",
      "Epoch 16/20\n",
      "575/575 [==============================] - 429s 747ms/step - loss: 0.6417 - accuracy: 0.7631 - val_loss: 1.0920 - val_accuracy: 0.6248\n",
      "Epoch 17/20\n",
      "575/575 [==============================] - 429s 746ms/step - loss: 0.6137 - accuracy: 0.7730 - val_loss: 1.0787 - val_accuracy: 0.6268\n",
      "Epoch 18/20\n",
      "575/575 [==============================] - 372s 648ms/step - loss: 0.5902 - accuracy: 0.7827 - val_loss: 1.0911 - val_accuracy: 0.6289\n",
      "Epoch 19/20\n",
      "575/575 [==============================] - 374s 650ms/step - loss: 0.5594 - accuracy: 0.7907 - val_loss: 1.1374 - val_accuracy: 0.6251\n",
      "Epoch 20/20\n",
      "575/575 [==============================] - 355s 618ms/step - loss: 0.5362 - accuracy: 0.8028 - val_loss: 1.1491 - val_accuracy: 0.6396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x142bc9d1f40>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=50,epochs=20,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5393f6a",
   "metadata": {},
   "source": [
    "# Saving the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38d7f2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ae48dadac362>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# serialize to JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjson_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"json_file_path\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m    \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# serialize weights to HDF5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# serialize to JSON\n",
    "json_file = model.to_json()\n",
    "with open(\"json_file_path\", \"w\") as file:\n",
    "   file.write(json_file)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"h5_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfa7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
