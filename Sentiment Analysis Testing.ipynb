{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a606eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a4ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_image=[\"train\",\"test\"]\n",
    "categories=[\"angry\",\"disgust\",\"fear\",\"happy\",\"neutral\",\"sad\",\"surprise\"]\n",
    "category_index={\"angry\":0,\"disgust\":1,\"fear\":2,\"happy\":3,\"neutral\":4,\"sad\":5,\"surprise\":6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c10b5",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437ace3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2795a8f6fa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "file = open('json_file_path', 'r')\n",
    "model_json = file.read()\n",
    "file.close()\n",
    "loaded_model = model_from_json(model_json)\n",
    "# load weights\n",
    "loaded_model.load_weights('h5_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a4381ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_44 (Conv2D)           (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 1,289,607\n",
      "Trainable params: 1,287,815\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b288e7",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7abdf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier( os.path.join(cv2.data.haarcascades, \"haarcascade_frontalface_default.xml\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a60f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "cap.set(3,640)\n",
    "cap.set(4,480)\n",
    "cap.set(10,180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f21d0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press the 'q' button in your keyboard to exit\n"
     ]
    }
   ],
   "source": [
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "    \n",
    "font=cv2.FONT_HERSHEY_COMPLEX\n",
    "categories=np.array(categories)\n",
    "\n",
    "while True:\n",
    "    success,result=cap.read()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"Can't recieve the signals....\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "    \n",
    "    grayimg=cv2.cvtColor(result,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces=faceCascade.detectMultiScale(\n",
    "    grayimg,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=4,\n",
    "    minSize=(30,30),\n",
    "    flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "    \n",
    "    img=result.copy()\n",
    "    img=cv2.resize(img,(48,48))\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Processed Image\",img)\n",
    "    \n",
    "    img=np.reshape(img,(1,48,48,1))\n",
    "    \n",
    "    #making our model to predict\n",
    "    predictions=loaded_model.predict(img)\n",
    "    prob=np.argmax(predictions[0])\n",
    "    category_prediction=categories[prob]\n",
    "    \n",
    "    \n",
    "    #this code is to make rectangles around the face\n",
    "    for (x,y,w,h) in faces:\n",
    "        if(category_prediction==\"angry\"):\n",
    "            color=(0,0,255)  #red color\n",
    "        elif(category_prediction==\"disgust\"):\n",
    "            color=(0,255,0)  #green color\n",
    "        elif(category_prediction==\"fear\"):\n",
    "            color=(0,0,0)    #black color\n",
    "        elif(category_prediction==\"happy\"):\n",
    "            color=(0,255,255) #yellow color\n",
    "        elif(category_prediction==\"neutral\"):\n",
    "            color=(153,255,255) #cream color\n",
    "        elif(category_prediction==\"sad\"):\n",
    "            color=(153,76,0)  #blue\n",
    "        elif(category_prediction==\"surprise\"):\n",
    "            color=(0,128,255) #orange\n",
    "        \n",
    "        cv2.rectangle(result,(x,y),(x+w,y+h),color,3)\n",
    "        cv2.rectangle(result,(x,y-40),(x+w,y),color,-1)\n",
    "        cv2.putText(result,str(((predictions[0][prob]*100)))+\"%\",(x+10,y-10),font,0.75,(255,255,255),2)\n",
    "        cv2.rectangle(result,(x,y+h+3),(x+w,y+h+40),color,-1)\n",
    "        cv2.putText(result,category_prediction,(x+10,y+h+45),font,0.75,(255,255,255),2)\n",
    "        \n",
    "        cv2.imshow(\"Result\",result)\n",
    "    if(cv2.waitKey(1) & 0xff==ord('q')):\n",
    "        break\n",
    "print(\"Press the 'q' button in your keyboard to exit\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13e79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
